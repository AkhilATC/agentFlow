"""
Documents â†’ Chunking â†’ Embeddings â†’ Vector Index â†’ Query â†’ LLM
DOC: OOH1-6MD FILES
"""
# LOAD CORPUS IN LLAMA ğŸ«¶
from llama_index.core import SimpleDirectoryReader,VectorStoreIndex
from llama_index.llms.ollama import Ollama
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding



# OLLAMA CONNECTION
Settings.llm = Ollama(model="llama3")
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
# load corpus in llama
print("[TAG] LOAD CORPUS IN LLAMA ğŸ’¾")
"""
ğŸ’¾ 8ï¸âƒ£ Persist Index (Important for Production)

Without this â†’ index rebuilds every run.
"""
documents  = SimpleDirectoryReader("corpus").load_data()
# print(documents)
print("[TAG] CREATE INDEX IN LLAMA ğŸ—‚ï¸")
# USE OLLAMA OR ANY OTHER EMMBEDINGS
# Build index
index = VectorStoreIndex.from_documents(documents)
print("[TAG] BUILD QUERY ENGINE IN LLAMA ğŸ—‚ï¸")
# Query engine build
query_engine = index.as_query_engine()
if __name__ == '__main__':
    while True:
        q = input("ASK ME [ğŸ¥·] ABOUT OOH IN KERALA : ")
        if q.lower() == "exit":
            break
        response = query_engine.query(q)
        print("ğŸ¤–", response)
